{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUfAcER1oUS6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PN9FvIx614"
      },
      "source": [
        "## Imports and Setup\n",
        "Let's start with the base imports for running this MediaPipe sample.\n",
        "\n",
        "*Notes:*\n",
        "\n",
        "* *If you see an error about flatbuffers incompatibility, it's fine to ignore it. MediaPipe requires a newer version of flatbuffers (v2), which is incompatible with the older version of Tensorflow (v2.9) currently preinstalled on Colab.*\n",
        "\n",
        "* *If you install MediaPipe outside of Colab, you only need to run pip install mediapipe. It isn't necessary to explicitly install flatbuffers*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxbHBsF-8Y_l"
      },
      "outputs": [],
      "source": [
        "!pip install -q flatbuffers==2.0.0\n",
        "!pip install -q sounddevice\n",
        "!pip install -q -i https://test.pypi.org/simple/ mediapipe==0.9.2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a49D7h4TVmru"
      },
      "source": [
        "## Download the interactive segmenter model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next thing you will need to do is download the interactive segmentation model that will be used for this demo. In this case you will use the **ptm_512_hdt_ptm_woid** model."
      ],
      "metadata": {
        "id": "QHXsuWDxpOj4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMjuVQiDYJKF"
      },
      "outputs": [],
      "source": [
        "#@title Start downloading here.\n",
        "!wget -O model.tflite -q https://storage.googleapis.com/mediapipe-assets/ptm_512_hdt_ptm_woid.tflite?generation=1678323604771164"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization utilities"
      ],
      "metadata": {
        "id": "fuf_ejsaKyjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better demonstrate the Interactive Segmenter API, we have created a set of visualization tools that will be used in this colab. These will draw an overlay for the selected item."
      ],
      "metadata": {
        "id": "vO-KVe1-jlHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _normalized_to_pixel_coordinates(\n",
        "    normalized_x: float, normalized_y: float, image_width: int,\n",
        "    image_height: int):\n",
        "  \"\"\"Converts normalized value pair to pixel coordinates.\"\"\"\n",
        "\n",
        "  # Checks if the float value is between 0 and 1.\n",
        "  def is_valid_normalized_value(value: float) -> bool:\n",
        "    return (value > 0 or math.isclose(0, value)) and (value < 1 or\n",
        "                                                      math.isclose(1, value))\n",
        "\n",
        "  if not (is_valid_normalized_value(normalized_x) and\n",
        "          is_valid_normalized_value(normalized_y)):\n",
        "    # TODO: Draw coordinates even if it's outside of the image bounds.\n",
        "    return None\n",
        "  x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
        "  y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
        "  return x_px, y_px"
      ],
      "metadata": {
        "id": "9gYTyXhTKy3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83PEJNp9yPBU"
      },
      "source": [
        "## Download a test image\n",
        "\n",
        "To demonstrate interactive segmentation, you can download a sample image using the following code.\n",
        "\n",
        "It's worth noting that while this is working with a single image, you can download a collection of images to store in the `IMAGE_FILENAMES` array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzXuqyIBlXer"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "IMAGE_FILENAMES = ['cats_and_dogs.jpg']\n",
        "\n",
        "for name in IMAGE_FILENAMES:\n",
        "  url = f'https://storage.googleapis.com/mediapipe-assets/{name}'\n",
        "  urllib.request.urlretrieve(url, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8XRmapjySMN"
      },
      "source": [
        "## Preview the downloaded image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also display the test image before using it with the interactive segmenter."
      ],
      "metadata": {
        "id": "Eu_q-Z03r-ed"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rjHk72-lmHX"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "\n",
        "# Height and width that will be used by the model\n",
        "DESIRED_HEIGHT = 480\n",
        "DESIRED_WIDTH = 480\n",
        "\n",
        "# Performs resizing and showing the image\n",
        "def resize_and_show(image):\n",
        "  h, w = image.shape[:2]\n",
        "  if h < w:\n",
        "    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
        "  else:\n",
        "    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
        "  cv2_imshow(img)\n",
        "\n",
        "\n",
        "# Preview the image(s)\n",
        "images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}\n",
        "for name, image in images.items():\n",
        "  print(name)\n",
        "  resize_and_show(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "## Running inference and visualizing the results\n",
        "To run inference using the interactive segmentation MediaPipe Task, you will need to initialize the `InteractiveSegmenter` using the model. This example will separate the background and foreground of the image and apply separate colors for them to highlight where each distinctive area exists. The interactive segmenter here will use a category mask, which applies a category to each found item based on confidence. You also provide a `RegionOfInterest`\n",
        "argument to the `segment` method."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = 0.68 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "y = 0.68 #@param {type:\"slider\", min:0, max:1, step:0.01}"
      ],
      "metadata": {
        "id": "hl6ojNjl4i6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl_Oiye4mUuo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "from mediapipe.tasks.python.components import containers\n",
        "\n",
        "\n",
        "BG_COLOR = (192, 192, 192) # gray\n",
        "MASK_COLOR = (255, 255, 255) # white\n",
        "\n",
        "OutputType = vision.InteractiveSegmenterOptions.OutputType\n",
        "RegionOfInterest = vision.InteractiveSegmenterRegionOfInterest\n",
        "NormalizedKeypoint = containers.keypoint.NormalizedKeypoint\n",
        "\n",
        "# Create the options that will be used for InteractiveSegmenter\n",
        "base_options = python.BaseOptions(model_asset_path='model.tflite')\n",
        "options = vision.ImageSegmenterOptions(base_options=base_options,\n",
        "                                              output_type=OutputType.CATEGORY_MASK)\n",
        "\n",
        "# Create the interactive segmenter\n",
        "with vision.InteractiveSegmenter.create_from_options(options) as segmenter:\n",
        "\n",
        "  # Loop through demo image(s)\n",
        "  for image_file_name in IMAGE_FILENAMES:\n",
        "\n",
        "    # Create the MediaPipe image file that will be segmented\n",
        "    image = mp.Image.create_from_file(image_file_name)\n",
        "\n",
        "    # Retrieve the masks for the segmented image\n",
        "    roi = RegionOfInterest(format=RegionOfInterest.Format.KEYPOINT,\n",
        "                           keypoint=NormalizedKeypoint(x, y))\n",
        "    category_masks = segmenter.segment(image, roi)\n",
        "\n",
        "    # Generate solid color images for showing the output segmentation mask.\n",
        "    image_data = image.numpy_view()\n",
        "    fg_image = np.zeros(image_data.shape, dtype=np.uint8)\n",
        "    fg_image[:] = MASK_COLOR\n",
        "    bg_image = np.zeros(image_data.shape, dtype=np.uint8)\n",
        "    bg_image[:] = BG_COLOR\n",
        "\n",
        "    condition = np.stack((category_masks[0].numpy_view(),) * 3, axis=-1) > 0.2\n",
        "    output_image = np.where(condition, fg_image, bg_image)\n",
        "\n",
        "    # Draw a circle to denote the point of interest\n",
        "    keypoint_px = _normalized_to_pixel_coordinates(x, y, image.width, image.height)\n",
        "    color, thickness, radius = (255, 255, 0), 10, 2\n",
        "    cv2.circle(output_image, keypoint_px, thickness, color, radius)\n",
        "\n",
        "    print(f'Segmentation mask of {name}:')\n",
        "    resize_and_show(output_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you know how to separate the foreground and background of an image, you can take it a step further and blur the background for an effect similar to what is provided by Google Hangouts."
      ],
      "metadata": {
        "id": "-RxsX2CjJbfo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciCGwCQ3gNDc"
      },
      "outputs": [],
      "source": [
        "# Blur the image background based on the segmentation mask.\n",
        "\n",
        "# Create the segmenter\n",
        "with python.vision.InteractiveSegmenter.create_from_options(options) as segmenter:\n",
        "\n",
        "  # Loop through available image(s)\n",
        "  for image_file_name in IMAGE_FILENAMES:\n",
        "\n",
        "    # Create the MediaPipe Image\n",
        "    image = mp.Image.create_from_file(image_file_name)\n",
        "\n",
        "    # Retrieve the category masks for the image\n",
        "    roi = RegionOfInterest(format=RegionOfInterest.Format.KEYPOINT,\n",
        "                           keypoint=NormalizedKeypoint(x, y))\n",
        "    category_masks = segmenter.segment(image, roi)\n",
        "\n",
        "    # Convert the BGR image to RGB\n",
        "    image_data = cv2.cvtColor(image.numpy_view(), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Apply effects\n",
        "    blurred_image = cv2.GaussianBlur(image_data, (55,55), 0)\n",
        "    condition = np.stack((category_masks[0].numpy_view(),) * 3, axis=-1) > 0.1\n",
        "    output_image = np.where(condition, image_data, blurred_image)\n",
        "\n",
        "    # Draw a circle to denote the point of interest\n",
        "    keypoint_px = _normalized_to_pixel_coordinates(x, y, image.width, image.height)\n",
        "    color, thickness, radius = (255, 255, 0), 10, 2\n",
        "    cv2.circle(output_image, keypoint_px, thickness, color, radius)\n",
        "\n",
        "    print(f'Blurred background of {image_file_name}:')\n",
        "    resize_and_show(output_image)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}